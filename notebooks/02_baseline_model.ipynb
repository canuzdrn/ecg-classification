{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cd0c215",
   "metadata": {},
   "source": [
    "## Task 2 — Modeling and Tuning (40 points)\n",
    "\n",
    "In this task, we develop machine learning models to classify univariate ECG time series into one of four classes: normal, atrial fibrillation (AF), other rhythms, and noisy.\n",
    "\n",
    "The modeling pipeline involves the following key components:\n",
    "- Designing at least two different model architectures\n",
    "- Training each model on the training set\n",
    "- Tuning hyperparameters such as learning rate, number of channels, optimizer, etc.\n",
    "- Evaluating performance using appropriate metrics (e.g., accuracy, macro F1-score)\n",
    "- Comparing model performance on the validation set\n",
    "- Generating predictions on the test data with the best-performing model\n",
    "\n",
    "We begin with the baseline model suggested in the exercise description:  \n",
    "A pipeline consisting of STFT → Conv2D → RNN → FC, which first transforms the ECG signals into the frequency domain, processes them with convolutional and recurrent layers, and finally outputs class probabilities through a fully connected layer.\n",
    "\n",
    "This baseline provides a strong foundation that leverages both frequency and temporal information in the ECG signals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d9367c",
   "metadata": {},
   "source": [
    "### ECGDataset: Custom PyTorch Dataset for ECG Signals\n",
    "\n",
    "We define a custom ECGDataset class to load and serve raw ECG signals for model training. This class inherits from torch.utils.data.Dataset and supports:\n",
    "\n",
    "- Index-based selection for validation splits\n",
    "- Conversion of variable-length 1d ECG signals (as numpy arrays) into tensor format\n",
    "- Association of each signal with its corresponding class label\n",
    "\n",
    "Since the ECG time series are univariate and vary in length, we keep each sample as an individual 1d tensor rather than padding them in the dataset. Padding will instead be handled dynamically at the batch level using prep_batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04bd6aa",
   "metadata": {},
   "source": [
    "### Training and Evaluation Functions\n",
    "\n",
    "We define two utility functions to encapsulate the training and evaluation logic for our model.\n",
    "\n",
    "train_one_epoch(model, dataloader, optimizer, loss_fn, device)\n",
    "\n",
    "This function performs one full training pass over the data:\n",
    "- Puts the model in training mode\n",
    "- Iterates over batches of data\n",
    "- Computes the forward pass and loss\n",
    "- Performs backpropagation and optimizer updates\n",
    "- Tracks predictions and computes accuracy and macro F1-score at the end\n",
    "\n",
    "It returns the average loss, accuracy, and F1-score for the epoch.\n",
    "\n",
    "evaluate(model, dataloader, loss_fn, device)\n",
    "\n",
    "This function evaluates the model on the validation set:\n",
    "- Runs in no-grad mode to avoid gradient tracking\n",
    "- Computes predictions and loss\n",
    "- Aggregates performance metrics (accuracy and macro F1-score)\n",
    "\n",
    "This separation ensures clean logging and enables early stopping or validation monitoring during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03b92389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c66aed85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique predictions: (array([0, 2]), array([962, 274]))\n",
      "Current learning rate: 0.0005\n",
      "Epoch 01 | Time: 27.6s\n",
      "  Train Loss: 1.3792 | Acc: 0.3953 | F1: 0.2510\n",
      "  Val   Loss: 1.3848 | Acc: 0.5437 | F1: 0.2450\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([259,   1, 958,  18]))\n",
      "Current learning rate: 0.0005\n",
      "Epoch 02 | Time: 27.2s\n",
      "  Train Loss: 1.3588 | Acc: 0.4758 | F1: 0.2606\n",
      "  Val   Loss: 1.3623 | Acc: 0.3511 | F1: 0.2121\n",
      "Unique predictions: (array([0, 1, 2]), array([1104,   40,   92]))\n",
      "Current learning rate: 0.0005\n",
      "Epoch 03 | Time: 27.3s\n",
      "  Train Loss: 1.3444 | Acc: 0.5112 | F1: 0.2684\n",
      "  Val   Loss: 1.3216 | Acc: 0.5906 | F1: 0.2753\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([162, 953, 120,   1]))\n",
      "Current learning rate: 0.0005\n",
      "Epoch 04 | Time: 27.2s\n",
      "  Train Loss: 1.3419 | Acc: 0.4653 | F1: 0.2590\n",
      "  Val   Loss: 1.3226 | Acc: 0.2015 | F1: 0.1635\n",
      "Unique predictions: (array([0, 2, 3]), array([962, 272,   2]))\n",
      "Current learning rate: 0.0005\n",
      "Epoch 05 | Time: 27.1s\n",
      "  Train Loss: 1.3299 | Acc: 0.5357 | F1: 0.2850\n",
      "  Val   Loss: 1.3426 | Acc: 0.5477 | F1: 0.2679\n",
      "Unique predictions: (array([0, 1, 2]), array([991,  71, 174]))\n",
      "Current learning rate: 0.0005\n",
      "Epoch 06 | Time: 27.0s\n",
      "  Train Loss: 1.3227 | Acc: 0.5153 | F1: 0.2871\n",
      "  Val   Loss: 1.3265 | Acc: 0.5583 | F1: 0.2879\n",
      "Unique predictions: (array([0, 1, 2]), array([991,  22, 223]))\n",
      "Current learning rate: 0.0005\n",
      "Epoch 07 | Time: 27.0s\n",
      "  Train Loss: 1.3285 | Acc: 0.5851 | F1: 0.2815\n",
      "  Val   Loss: 1.3115 | Acc: 0.5639 | F1: 0.2891\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([956,  61, 217,   2]))\n",
      "Current learning rate: 0.0005\n",
      "Epoch 08 | Time: 27.2s\n",
      "  Train Loss: 1.3292 | Acc: 0.5375 | F1: 0.2904\n",
      "  Val   Loss: 1.3094 | Acc: 0.5445 | F1: 0.2946\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1069,   87,   58,   22]))\n",
      "Current learning rate: 0.0005\n",
      "Epoch 09 | Time: 27.3s\n",
      "  Train Loss: 1.3335 | Acc: 0.5565 | F1: 0.2752\n",
      "  Val   Loss: 1.3151 | Acc: 0.5761 | F1: 0.3055\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1010,   37,  104,   85]))\n",
      "Current learning rate: 0.0005\n",
      "Epoch 10 | Time: 27.4s\n",
      "  Train Loss: 1.3221 | Acc: 0.5317 | F1: 0.2936\n",
      "  Val   Loss: 1.3517 | Acc: 0.5534 | F1: 0.2832\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([979,  93, 160,   4]))\n",
      "Current learning rate: 0.0005\n",
      "Epoch 11 | Time: 27.3s\n",
      "  Train Loss: 1.3240 | Acc: 0.5788 | F1: 0.2952\n",
      "  Val   Loss: 1.3089 | Acc: 0.5534 | F1: 0.3060\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([947,   5, 283,   1]))\n",
      "Current learning rate: 0.0005\n",
      "Epoch 12 | Time: 27.4s\n",
      "  Train Loss: 1.3233 | Acc: 0.5533 | F1: 0.2929\n",
      "  Val   Loss: 1.3365 | Acc: 0.5453 | F1: 0.2631\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([520, 130, 209, 377]))\n",
      "Current learning rate: 0.0005\n",
      "Epoch 13 | Time: 27.6s\n",
      "  Train Loss: 1.2022 | Acc: 0.5454 | F1: 0.3405\n",
      "  Val   Loss: 1.2133 | Acc: 0.4175 | F1: 0.3101\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([852,  81, 270,  33]))\n",
      "Current learning rate: 0.0005\n",
      "Epoch 14 | Time: 28.4s\n",
      "  Train Loss: 1.1495 | Acc: 0.5596 | F1: 0.3699\n",
      "  Val   Loss: 1.1627 | Acc: 0.5761 | F1: 0.4512\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([585, 112,  69, 470]))\n",
      "Current learning rate: 0.0005\n",
      "Epoch 15 | Time: 28.9s\n",
      "  Train Loss: 1.1262 | Acc: 0.5675 | F1: 0.3807\n",
      "  Val   Loss: 1.3457 | Acc: 0.4134 | F1: 0.2733\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([849,  78, 268,  41]))\n",
      "Current learning rate: 0.0005\n",
      "Epoch 16 | Time: 26.9s\n",
      "  Train Loss: 1.0877 | Acc: 0.5711 | F1: 0.4111\n",
      "  Val   Loss: 1.1149 | Acc: 0.6286 | F1: 0.5340\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([660, 134, 284, 158]))\n",
      "Current learning rate: 0.0005\n",
      "Epoch 17 | Time: 26.4s\n",
      "  Train Loss: 1.0794 | Acc: 0.5776 | F1: 0.4329\n",
      "  Val   Loss: 1.0547 | Acc: 0.5518 | F1: 0.4356\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([854, 141,  87, 154]))\n",
      "Current learning rate: 0.0005\n",
      "Epoch 18 | Time: 26.3s\n",
      "  Train Loss: 1.0771 | Acc: 0.5634 | F1: 0.4295\n",
      "  Val   Loss: 1.0580 | Acc: 0.5639 | F1: 0.3821\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([870, 112, 202,  52]))\n",
      "Current learning rate: 0.0005\n",
      "Epoch 19 | Time: 27.6s\n",
      "  Train Loss: 1.0430 | Acc: 0.5768 | F1: 0.4464\n",
      "  Val   Loss: 1.0319 | Acc: 0.6481 | F1: 0.5449\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([832, 159, 138, 107]))\n",
      "Current learning rate: 0.0005\n",
      "Epoch 20 | Time: 27.5s\n",
      "  Train Loss: 1.0480 | Acc: 0.5454 | F1: 0.4346\n",
      "  Val   Loss: 1.0171 | Acc: 0.5995 | F1: 0.4553\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([863, 154, 133,  86]))\n",
      "Current learning rate: 0.0005\n",
      "Epoch 21 | Time: 27.3s\n",
      "  Train Loss: 1.0240 | Acc: 0.5677 | F1: 0.4499\n",
      "  Val   Loss: 0.9993 | Acc: 0.6206 | F1: 0.4843\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([790, 163, 192,  91]))\n",
      "Current learning rate: 0.00025\n",
      "Epoch 22 | Time: 27.2s\n",
      "  Train Loss: 0.9984 | Acc: 0.5729 | F1: 0.4677\n",
      "  Val   Loss: 0.9871 | Acc: 0.6262 | F1: 0.5109\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([722, 227, 198,  89]))\n",
      "Current learning rate: 0.00025\n",
      "Epoch 23 | Time: 26.7s\n",
      "  Train Loss: 0.9629 | Acc: 0.5865 | F1: 0.4769\n",
      "  Val   Loss: 0.9388 | Acc: 0.6238 | F1: 0.5169\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([692, 231, 205, 108]))\n",
      "Current learning rate: 0.00025\n",
      "Epoch 24 | Time: 26.3s\n",
      "  Train Loss: 0.9443 | Acc: 0.5895 | F1: 0.4795\n",
      "  Val   Loss: 0.9226 | Acc: 0.6206 | F1: 0.5152\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([437, 302, 244, 253]))\n",
      "Current learning rate: 0.000125\n",
      "Epoch 25 | Time: 26.2s\n",
      "  Train Loss: 0.9305 | Acc: 0.5909 | F1: 0.4869\n",
      "  Val   Loss: 1.0662 | Acc: 0.4717 | F1: 0.3982\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([743, 204, 194,  95]))\n",
      "Current learning rate: 0.000125\n",
      "Epoch 26 | Time: 26.5s\n",
      "  Train Loss: 0.9080 | Acc: 0.6057 | F1: 0.5017\n",
      "  Val   Loss: 0.9040 | Acc: 0.6416 | F1: 0.5338\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([633, 248, 193, 162]))\n",
      "Current learning rate: 0.000125\n",
      "Epoch 27 | Time: 27.8s\n",
      "  Train Loss: 0.8833 | Acc: 0.6170 | F1: 0.5143\n",
      "  Val   Loss: 0.9289 | Acc: 0.5728 | F1: 0.4586\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([664, 236, 232, 104]))\n",
      "Current learning rate: 6.25e-05\n",
      "Epoch 28 | Time: 26.9s\n",
      "  Train Loss: 0.8673 | Acc: 0.6140 | F1: 0.5136\n",
      "  Val   Loss: 0.8704 | Acc: 0.6230 | F1: 0.5236\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([619, 248, 265, 104]))\n",
      "Current learning rate: 6.25e-05\n",
      "Epoch 29 | Time: 26.9s\n",
      "  Train Loss: 0.8448 | Acc: 0.6330 | F1: 0.5383\n",
      "  Val   Loss: 0.8770 | Acc: 0.6003 | F1: 0.5097\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([770, 192, 189,  85]))\n",
      "Current learning rate: 6.25e-05\n",
      "Epoch 30 | Time: 26.7s\n",
      "  Train Loss: 0.8352 | Acc: 0.6282 | F1: 0.5353\n",
      "  Val   Loss: 0.8779 | Acc: 0.6610 | F1: 0.5486\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([731, 202, 207,  96]))\n",
      "Current learning rate: 6.25e-05\n",
      "Epoch 31 | Time: 27.5s\n",
      "  Train Loss: 0.8224 | Acc: 0.6280 | F1: 0.5400\n",
      "  Val   Loss: 0.8712 | Acc: 0.6513 | F1: 0.5407\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([484, 318, 308, 126]))\n",
      "Current learning rate: 6.25e-05\n",
      "Epoch 32 | Time: 27.0s\n",
      "  Train Loss: 0.8285 | Acc: 0.6421 | F1: 0.5488\n",
      "  Val   Loss: 0.9063 | Acc: 0.5210 | F1: 0.4614\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([672, 199, 237, 128]))\n",
      "Current learning rate: 3.125e-05\n",
      "Epoch 33 | Time: 26.8s\n",
      "  Train Loss: 0.8158 | Acc: 0.6397 | F1: 0.5484\n",
      "  Val   Loss: 0.8737 | Acc: 0.6173 | F1: 0.5052\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([632, 241, 275,  88]))\n",
      "Current learning rate: 3.125e-05\n",
      "Epoch 34 | Time: 26.9s\n",
      "  Train Loss: 0.8098 | Acc: 0.6502 | F1: 0.5574\n",
      "  Val   Loss: 0.8595 | Acc: 0.6181 | F1: 0.5331\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([755, 176, 215,  90]))\n",
      "Current learning rate: 3.125e-05\n",
      "Epoch 35 | Time: 26.9s\n",
      "  Train Loss: 0.8035 | Acc: 0.6409 | F1: 0.5525\n",
      "  Val   Loss: 0.8651 | Acc: 0.6659 | F1: 0.5533\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([662, 230, 238, 106]))\n",
      "Current learning rate: 3.125e-05\n",
      "Epoch 36 | Time: 26.9s\n",
      "  Train Loss: 0.7949 | Acc: 0.6516 | F1: 0.5658\n",
      "  Val   Loss: 0.8591 | Acc: 0.6254 | F1: 0.5255\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([730, 206, 218,  82]))\n",
      "Current learning rate: 3.125e-05\n",
      "Epoch 37 | Time: 26.9s\n",
      "  Train Loss: 0.7927 | Acc: 0.6518 | F1: 0.5637\n",
      "  Val   Loss: 0.8588 | Acc: 0.6513 | F1: 0.5518\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([678, 224, 234, 100]))\n",
      "Current learning rate: 1.5625e-05\n",
      "Epoch 38 | Time: 27.3s\n",
      "  Train Loss: 0.7882 | Acc: 0.6516 | F1: 0.5619\n",
      "  Val   Loss: 0.8542 | Acc: 0.6294 | F1: 0.5282\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([648, 236, 252, 100]))\n",
      "Current learning rate: 1.5625e-05\n",
      "Epoch 39 | Time: 27.5s\n",
      "  Train Loss: 0.7800 | Acc: 0.6502 | F1: 0.5617\n",
      "  Val   Loss: 0.8532 | Acc: 0.6230 | F1: 0.5251\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([681, 213, 251,  91]))\n",
      "Current learning rate: 1.5625e-05\n",
      "Epoch 40 | Time: 26.9s\n",
      "  Train Loss: 0.7738 | Acc: 0.6541 | F1: 0.5688\n",
      "  Val   Loss: 0.8522 | Acc: 0.6327 | F1: 0.5368\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import data\n",
    "from torch.utils.data import DataLoader\n",
    "from src.parser import read_zip_binary\n",
    "from src.train import train_model\n",
    "from src.train_utils import train_one_epoch, evaluate\n",
    "from src.ecg_dataset import ECGDataset, prep_batch\n",
    "from src.stft_baseline import BaselineSTFTModel\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# load training data\n",
    "X_train = read_zip_binary(\"../data/X_train.zip\")\n",
    "\n",
    "# load training labels\n",
    "y_train = pd.read_csv(\"../data/y_train.csv\", header=None)\n",
    "y_train.columns = [\"y\"]\n",
    "\n",
    "# load split index\n",
    "train_idx = np.load(\"../data/train_idx.npy\")\n",
    "val_idx = np.load(\"../data/val_idx.npy\")\n",
    "\n",
    "# dataloader\n",
    "train_dataset = ECGDataset(X_train, y_train, indices=train_idx)\n",
    "val_dataset = ECGDataset(X_train, y_train, indices=val_idx)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=prep_batch)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=prep_batch)\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "# models is baseline stft model right now\n",
    "model = BaselineSTFTModel().to(device)\n",
    "\n",
    "# loss and optimizer\n",
    "\n",
    "# weights to avoid one class collapse\n",
    "weights = compute_class_weight(class_weight=\"balanced\", classes=np.array([0, 1, 2, 3]), y=y_train[\"y\"])\n",
    "weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# careful with the learning rate\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=5e-4,\n",
    "    weight_decay=1e-2  # L2 regularization to prevent overfitting\n",
    ")\n",
    "\n",
    "# train model\n",
    "model = train_model(model, train_loader, val_loader, optimizer, loss_fn, device, num_epochs=40)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg-amls-venvironment",
   "language": "python",
   "name": "eeg-amls-venvironment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
