{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cd0c215",
   "metadata": {},
   "source": [
    "## Task 2 — Modeling and Tuning (40 points)\n",
    "\n",
    "In this task, we develop machine learning models to classify univariate ECG time series into one of four classes: normal, atrial fibrillation (AF), other rhythms, and noisy.\n",
    "\n",
    "The modeling pipeline involves the following key components:\n",
    "- Designing at least two different model architectures\n",
    "- Training each model on the training set\n",
    "- Tuning hyperparameters such as learning rate, number of channels, optimizer, etc.\n",
    "- Evaluating performance using appropriate metrics (e.g., accuracy, macro F1-score)\n",
    "- Comparing model performance on the validation set\n",
    "- Generating predictions on the test data with the best-performing model\n",
    "\n",
    "We begin with the baseline model suggested in the exercise description:  \n",
    "A pipeline consisting of STFT → Conv2D → RNN → FC, which first transforms the ECG signals into the frequency domain, processes them with convolutional and recurrent layers, and finally outputs class probabilities through a fully connected layer.\n",
    "\n",
    "This baseline provides a strong foundation that leverages both frequency and temporal information in the ECG signals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d9367c",
   "metadata": {},
   "source": [
    "### ECGDataset: Custom PyTorch Dataset for ECG Signals\n",
    "\n",
    "We define a custom ECGDataset class to load and serve raw ECG signals for model training. This class inherits from torch.utils.data.Dataset and supports:\n",
    "\n",
    "- Index-based selection for validation splits\n",
    "- Conversion of variable-length 1d ECG signals (as numpy arrays) into tensor format\n",
    "- Association of each signal with its corresponding class label\n",
    "\n",
    "Since the ECG time series are univariate and vary in length, we keep each sample as an individual 1d tensor rather than padding them in the dataset. Padding will instead be handled dynamically at the batch level using prep_batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04bd6aa",
   "metadata": {},
   "source": [
    "### Training and Evaluation Functions\n",
    "\n",
    "We define two utility functions to encapsulate the training and evaluation logic for our model.\n",
    "\n",
    "train_one_epoch(model, dataloader, optimizer, loss_fn, device)\n",
    "\n",
    "This function performs one full training pass over the data:\n",
    "- Puts the model in training mode\n",
    "- Iterates over batches of data\n",
    "- Computes the forward pass and loss\n",
    "- Performs backpropagation and optimizer updates\n",
    "- Tracks predictions and computes accuracy and macro F1-score at the end\n",
    "\n",
    "It returns the average loss, accuracy, and F1-score for the epoch.\n",
    "\n",
    "evaluate(model, dataloader, loss_fn, device)\n",
    "\n",
    "This function evaluates the model on the validation set:\n",
    "- Runs in no-grad mode to avoid gradient tracking\n",
    "- Computes predictions and loss\n",
    "- Aggregates performance metrics (accuracy and macro F1-score)\n",
    "\n",
    "This separation ensures clean logging and enables early stopping or validation monitoring during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03b92389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c66aed85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cuzdu/Desktop/early-alzhemir-detection/env/lib/python3.12/site-packages/torch/functional.py:704: UserWarning: A window was not provided. A rectangular window will be applied,which is known to cause spectral leakage. Other windows such as torch.hann_window or torch.hamming_window can are recommended to reduce spectral leakage.To suppress this warning and use a rectangular window, explicitly set `window=torch.ones(n_fft, device=<device>)`. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/SpectralOps.cpp:842.)\n",
      "  return _VF.stft(  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique predictions: (array([0, 1, 2, 3]), array([972,   2, 243,  19]))\n",
      "Epoch 01 | Time: 52.5s\n",
      "  Train Loss: 1.3792 | Acc: 0.4368 | F1: 0.2540\n",
      "  Val   Loss: 1.3693 | Acc: 0.5429 | F1: 0.2506\n",
      "Unique predictions: (array([0, 1]), array([1002,  234]))\n",
      "Epoch 02 | Time: 54.7s\n",
      "  Train Loss: 1.3780 | Acc: 0.4934 | F1: 0.2402\n",
      "  Val   Loss: 1.3802 | Acc: 0.5113 | F1: 0.2129\n",
      "Unique predictions: (array([0, 2, 3]), array([973, 245,  18]))\n",
      "Epoch 03 | Time: 56.2s\n",
      "  Train Loss: 1.3855 | Acc: 0.5027 | F1: 0.2415\n",
      "  Val   Loss: 1.3634 | Acc: 0.5477 | F1: 0.2615\n",
      "Unique predictions: (array([0, 1, 2]), array([965,   2, 269]))\n",
      "Epoch 04 | Time: 59.4s\n",
      "  Train Loss: 1.3761 | Acc: 0.5128 | F1: 0.2502\n",
      "  Val   Loss: 1.3629 | Acc: 0.5453 | F1: 0.2458\n",
      "Unique predictions: (array([0, 1]), array([985, 251]))\n",
      "Epoch 05 | Time: 68.6s\n",
      "  Train Loss: 1.3800 | Acc: 0.5420 | F1: 0.2412\n",
      "  Val   Loss: 1.3736 | Acc: 0.5024 | F1: 0.2097\n",
      "Unique predictions: (array([0]), array([1236]))\n",
      "Epoch 06 | Time: 69.9s\n",
      "  Train Loss: 1.3962 | Acc: 0.4590 | F1: 0.2559\n",
      "  Val   Loss: 1.4009 | Acc: 0.5890 | F1: 0.1853\n",
      "Unique predictions: (array([0, 1, 2]), array([ 19, 691, 526]))\n",
      "Epoch 07 | Time: 66.6s\n",
      "  Train Loss: 1.4201 | Acc: 0.3500 | F1: 0.2444\n",
      "  Val   Loss: 1.3745 | Acc: 0.1796 | F1: 0.1304\n",
      "Unique predictions: (array([0, 1, 3]), array([232, 275, 729]))\n",
      "Epoch 08 | Time: 66.8s\n",
      "  Train Loss: 1.3987 | Acc: 0.3152 | F1: 0.2381\n",
      "  Val   Loss: 1.4161 | Acc: 0.1375 | F1: 0.1103\n",
      "Unique predictions: (array([0, 2, 3]), array([ 68, 232, 936]))\n",
      "Epoch 09 | Time: 65.8s\n",
      "  Train Loss: 1.3776 | Acc: 0.3688 | F1: 0.2534\n",
      "  Val   Loss: 1.3673 | Acc: 0.1319 | F1: 0.1162\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([740,  20, 251, 225]))\n",
      "Epoch 10 | Time: 67.0s\n",
      "  Train Loss: 1.3625 | Acc: 0.3449 | F1: 0.2528\n",
      "  Val   Loss: 1.3485 | Acc: 0.4523 | F1: 0.2655\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import data\n",
    "from torch.utils.data import DataLoader\n",
    "from src.parser import read_zip_binary\n",
    "from src.train import train_model\n",
    "from src.train_utils import train_one_epoch, evaluate\n",
    "from src.ecg_dataset import ECGDataset, prep_batch\n",
    "from src.stft_baseline import BaselineSTFTModel\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# load training data\n",
    "X_train = read_zip_binary(\"../data/X_train.zip\")\n",
    "\n",
    "# load training labels\n",
    "y_train = pd.read_csv(\"../data/y_train.csv\", header=None)\n",
    "y_train.columns = [\"y\"]\n",
    "\n",
    "# load split index\n",
    "train_idx = np.load(\"../data/train_idx.npy\")\n",
    "val_idx = np.load(\"../data/val_idx.npy\")\n",
    "\n",
    "# dataloader\n",
    "train_dataset = ECGDataset(X_train, y_train, indices=train_idx)\n",
    "val_dataset = ECGDataset(X_train, y_train, indices=val_idx)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=prep_batch)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=prep_batch)\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "# models is baseline stft model right now\n",
    "model = BaselineSTFTModel().to(device)\n",
    "\n",
    "# loss and optimizer\n",
    "\n",
    "# weights to avoid one class collapse\n",
    "weights = compute_class_weight(class_weight=\"balanced\", classes=[0, 1, 2, 3], y=y_train[\"y\"])\n",
    "weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# careful with the learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "\n",
    "# train model\n",
    "model = train_model(model, train_loader, val_loader, optimizer, loss_fn, device, num_epochs=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
