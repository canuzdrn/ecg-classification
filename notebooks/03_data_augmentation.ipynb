{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84646644",
   "metadata": {},
   "source": [
    "## Task 3 — Data Augmentation and Feature Engineering (30 points)\n",
    "> First Practice: Time‑Compression Data Augmentation\n",
    "\n",
    "- Our first technique for data augmentation is to create an additional copy of each ECG signal by speeding it up (compressing its duration by 20%). Following the trajectory:\n",
    "   - Load originals, reading our raw signals into `X_orig`.  \n",
    "   - Compression for each `s` in `X_orig`, apply `time_compress(s, rate=0.8)` → `X_comp`.  \n",
    "   - Augmenting, concatenate `X_all = X_orig + X_comp` and duplicate labels `y_all = [y_orig; y_orig]`.  \n",
    "   - Splitting, run a **stratified** train/validation split on `(X_all, y_all)` to preserve class balance.  \n",
    "- Reasoning\n",
    "   - **Variation in heart rate:** Simulates faster‑than‑normal beats, making the model robust to timing shifts.  \n",
    "   - **Increases data volume:** Doubles your training set without collecting new recordings.  \n",
    "   - **Preserves labels:** Time‑compression does not alter arrhythmia class.  \n",
    "\n",
    "> Second Practice:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7289e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95ec2410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def time_compress(signal, rate=0.8):\n",
    "    \"\"\"\n",
    "    Applies time compression to a 1D time series signal.\n",
    "\n",
    "    This technique, often referred to as time stretching/compression or time warping,\n",
    "    simulates natural variations in the speed or duration of events within a time series.\n",
    "    By altering the temporal dimension, it helps the model become more robust to the\n",
    "    precise \"sampling\" or timing of the signal, enhancing its invariance to time deformation.[1]\n",
    "\n",
    "    Args:\n",
    "        signal (np.ndarray): The input 1D time series signal.\n",
    "        rate (float): The compression rate. A rate < 1.0 compresses the signal\n",
    "                      (makes it shorter/faster), while a rate > 1.0 stretches it\n",
    "                      (makes it longer/slower). Default is 0.8 for compression.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The time-compressed signal.\n",
    "    \"\"\"\n",
    "    original_length = len(signal)\n",
    "    new_length = int(original_length * rate)\n",
    "\n",
    "    # Create original time points (indices)\n",
    "    original_time_points = np.arange(original_length)\n",
    "\n",
    "    # Create new time points for the compressed signal\n",
    "    # These points will span the original signal's \"time\" but with a new number of steps\n",
    "    new_time_points = np.linspace(0, original_length - 1, new_length)\n",
    "\n",
    "    # Create an interpolation function based on the original signal\n",
    "    # 'linear' interpolation is a common and simple choice for time series.\n",
    "    interpolator = interp1d(original_time_points, signal, kind='linear', fill_value=\"extrapolate\")\n",
    "\n",
    "    # Apply the interpolation to the new time points\n",
    "    compressed_signal = interpolator(new_time_points)\n",
    "\n",
    "    return compressed_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8233e560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique predictions: (array([0, 1, 2]), array([1372,    1, 1099]))\n",
      "Epoch 01 | Time: 52.2s\n",
      "  Train Loss: 1.4056 | Acc: 0.3224 | F1: 0.2321\n",
      "  Val   Loss: 1.3719 | Acc: 0.4814 | F1: 0.2453\n",
      "Unique predictions: (array([0, 1, 2]), array([2153,    2,  317]))\n",
      "Epoch 02 | Time: 51.3s\n",
      "  Train Loss: 1.3922 | Acc: 0.3660 | F1: 0.2467\n",
      "  Val   Loss: 1.3578 | Acc: 0.5740 | F1: 0.2380\n",
      "Unique predictions: (array([0, 1, 2]), array([2168,   62,  242]))\n",
      "Epoch 03 | Time: 50.8s\n",
      "  Train Loss: 1.3664 | Acc: 0.3661 | F1: 0.2546\n",
      "  Val   Loss: 1.3274 | Acc: 0.5688 | F1: 0.2434\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1386,  408,  326,  352]))\n",
      "Epoch 04 | Time: 50.7s\n",
      "  Train Loss: 1.2607 | Acc: 0.4060 | F1: 0.3141\n",
      "  Val   Loss: 1.1109 | Acc: 0.4680 | F1: 0.3566\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1814,  238,  155,  265]))\n",
      "Epoch 05 | Time: 51.4s\n",
      "  Train Loss: 1.1216 | Acc: 0.4249 | F1: 0.3562\n",
      "  Val   Loss: 1.0889 | Acc: 0.5534 | F1: 0.3830\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1557,  494,  272,  149]))\n",
      "Epoch 06 | Time: 50.7s\n",
      "  Train Loss: 1.0812 | Acc: 0.4356 | F1: 0.3744\n",
      "  Val   Loss: 1.1149 | Acc: 0.5405 | F1: 0.4267\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1242,  577,  307,  346]))\n",
      "Epoch 07 | Time: 51.1s\n",
      "  Train Loss: 1.0514 | Acc: 0.4728 | F1: 0.3951\n",
      "  Val   Loss: 1.0382 | Acc: 0.4911 | F1: 0.3929\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1725,  438,  100,  209]))\n",
      "Epoch 08 | Time: 50.5s\n",
      "  Train Loss: 1.0142 | Acc: 0.4995 | F1: 0.4223\n",
      "  Val   Loss: 1.0169 | Acc: 0.5951 | F1: 0.4338\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1397,  429,  330,  316]))\n",
      "Epoch 09 | Time: 50.5s\n",
      "  Train Loss: 0.9895 | Acc: 0.5358 | F1: 0.4485\n",
      "  Val   Loss: 0.9636 | Acc: 0.5732 | F1: 0.4527\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1714,  223,  296,  239]))\n",
      "Epoch 10 | Time: 50.8s\n",
      "  Train Loss: 0.9573 | Acc: 0.5730 | F1: 0.4728\n",
      "  Val   Loss: 0.9694 | Acc: 0.6477 | F1: 0.5012\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1103,  531,  696,  142]))\n",
      "Epoch 11 | Time: 50.2s\n",
      "  Train Loss: 0.9398 | Acc: 0.5907 | F1: 0.4892\n",
      "  Val   Loss: 0.9627 | Acc: 0.5785 | F1: 0.5142\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([884, 676, 705, 207]))\n",
      "Epoch 12 | Time: 49.6s\n",
      "  Train Loss: 0.8938 | Acc: 0.6210 | F1: 0.5157\n",
      "  Val   Loss: 0.9420 | Acc: 0.5279 | F1: 0.4820\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1415,  433,  444,  180]))\n",
      "Epoch 13 | Time: 49.4s\n",
      "  Train Loss: 0.8871 | Acc: 0.6198 | F1: 0.5212\n",
      "  Val   Loss: 0.8862 | Acc: 0.6436 | F1: 0.5367\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1331,  523,  327,  291]))\n",
      "Epoch 14 | Time: 49.6s\n",
      "  Train Loss: 0.8839 | Acc: 0.6259 | F1: 0.5256\n",
      "  Val   Loss: 0.8421 | Acc: 0.6234 | F1: 0.5008\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1249,  576,  457,  190]))\n",
      "Epoch 15 | Time: 49.9s\n",
      "  Train Loss: 0.8517 | Acc: 0.6422 | F1: 0.5412\n",
      "  Val   Loss: 0.8348 | Acc: 0.6189 | F1: 0.5279\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1128,  473,  623,  248]))\n",
      "Epoch 16 | Time: 49.7s\n",
      "  Train Loss: 0.8370 | Acc: 0.6514 | F1: 0.5518\n",
      "  Val   Loss: 0.8302 | Acc: 0.6242 | F1: 0.5441\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1247,  347,  651,  227]))\n",
      "Epoch 17 | Time: 52.2s\n",
      "  Train Loss: 0.8195 | Acc: 0.6443 | F1: 0.5483\n",
      "  Val   Loss: 0.8013 | Acc: 0.6618 | F1: 0.5747\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1446,  579,  171,  276]))\n",
      "Epoch 18 | Time: 54.7s\n",
      "  Train Loss: 0.7986 | Acc: 0.6595 | F1: 0.5650\n",
      "  Val   Loss: 0.8091 | Acc: 0.6351 | F1: 0.4936\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1544,  462,  205,  261]))\n",
      "Epoch 19 | Time: 52.0s\n",
      "  Train Loss: 0.7789 | Acc: 0.6724 | F1: 0.5808\n",
      "  Val   Loss: 0.7837 | Acc: 0.6675 | F1: 0.5215\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([805, 977, 478, 212]))\n",
      "Epoch 20 | Time: 50.5s\n",
      "  Train Loss: 0.7497 | Acc: 0.6855 | F1: 0.5958\n",
      "  Val   Loss: 0.9299 | Acc: 0.4854 | F1: 0.4577\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1371,  645,  248,  208]))\n",
      "Epoch 21 | Time: 49.8s\n",
      "  Train Loss: 0.7342 | Acc: 0.6928 | F1: 0.6119\n",
      "  Val   Loss: 0.7574 | Acc: 0.6444 | F1: 0.5316\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([949, 908, 379, 236]))\n",
      "Epoch 22 | Time: 49.7s\n",
      "  Train Loss: 0.7108 | Acc: 0.6908 | F1: 0.6127\n",
      "  Val   Loss: 0.8849 | Acc: 0.5267 | F1: 0.4785\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1437,  140,  748,  147]))\n",
      "Epoch 23 | Time: 49.8s\n",
      "  Train Loss: 0.6973 | Acc: 0.7011 | F1: 0.6243\n",
      "  Val   Loss: 0.7742 | Acc: 0.7395 | F1: 0.6516\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1574,  251,  440,  207]))\n",
      "Epoch 24 | Time: 49.8s\n",
      "  Train Loss: 0.6940 | Acc: 0.6946 | F1: 0.6193\n",
      "  Val   Loss: 0.7098 | Acc: 0.7476 | F1: 0.6465\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1461,  555,  297,  159]))\n",
      "Epoch 25 | Time: 50.0s\n",
      "  Train Loss: 0.6676 | Acc: 0.7075 | F1: 0.6362\n",
      "  Val   Loss: 0.7277 | Acc: 0.6804 | F1: 0.5779\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1308,  428,  585,  151]))\n",
      "Epoch 26 | Time: 50.7s\n",
      "  Train Loss: 0.6538 | Acc: 0.7152 | F1: 0.6443\n",
      "  Val   Loss: 0.6963 | Acc: 0.7019 | F1: 0.6346\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1063,  387,  760,  262]))\n",
      "Epoch 27 | Time: 49.8s\n",
      "  Train Loss: 0.6270 | Acc: 0.7206 | F1: 0.6585\n",
      "  Val   Loss: 0.7157 | Acc: 0.6561 | F1: 0.5929\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1204,  540,  574,  154]))\n",
      "Epoch 28 | Time: 49.5s\n",
      "  Train Loss: 0.6116 | Acc: 0.7220 | F1: 0.6589\n",
      "  Val   Loss: 0.7010 | Acc: 0.6748 | F1: 0.6111\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1398,  439,  552,   83]))\n",
      "Epoch 29 | Time: 49.8s\n",
      "  Train Loss: 0.6165 | Acc: 0.7282 | F1: 0.6652\n",
      "  Val   Loss: 0.8396 | Acc: 0.7189 | F1: 0.6336\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1199,  445,  606,  222]))\n",
      "Epoch 30 | Time: 49.6s\n",
      "  Train Loss: 0.6004 | Acc: 0.7302 | F1: 0.6764\n",
      "  Val   Loss: 0.6798 | Acc: 0.6812 | F1: 0.6120\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1393,  469,  433,  177]))\n",
      "Epoch 31 | Time: 50.1s\n",
      "  Train Loss: 0.5894 | Acc: 0.7420 | F1: 0.6850\n",
      "  Val   Loss: 0.6655 | Acc: 0.7168 | F1: 0.6253\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1209,  509,  552,  202]))\n",
      "Epoch 32 | Time: 49.3s\n",
      "  Train Loss: 0.5691 | Acc: 0.7390 | F1: 0.6897\n",
      "  Val   Loss: 0.6661 | Acc: 0.6837 | F1: 0.6101\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1439,  320,  535,  178]))\n",
      "Epoch 33 | Time: 49.7s\n",
      "  Train Loss: 0.5558 | Acc: 0.7501 | F1: 0.6978\n",
      "  Val   Loss: 0.6277 | Acc: 0.7528 | F1: 0.6748\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1548,  336,  419,  169]))\n",
      "Epoch 34 | Time: 49.5s\n",
      "  Train Loss: 0.5358 | Acc: 0.7594 | F1: 0.7197\n",
      "  Val   Loss: 0.6752 | Acc: 0.7585 | F1: 0.6739\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1199,  459,  637,  177]))\n",
      "Epoch 35 | Time: 49.7s\n",
      "  Train Loss: 0.5228 | Acc: 0.7589 | F1: 0.7215\n",
      "  Val   Loss: 0.6642 | Acc: 0.6938 | F1: 0.6324\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1326,  384,  562,  200]))\n",
      "Epoch 36 | Time: 49.6s\n",
      "  Train Loss: 0.5120 | Acc: 0.7620 | F1: 0.7272\n",
      "  Val   Loss: 0.6365 | Acc: 0.7334 | F1: 0.6552\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1421,  400,  494,  157]))\n",
      "Epoch 37 | Time: 49.9s\n",
      "  Train Loss: 0.5156 | Acc: 0.7723 | F1: 0.7336\n",
      "  Val   Loss: 0.6572 | Acc: 0.7447 | F1: 0.6685\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1465,  340,  524,  143]))\n",
      "Epoch 38 | Time: 49.4s\n",
      "  Train Loss: 0.4898 | Acc: 0.7749 | F1: 0.7401\n",
      "  Val   Loss: 0.6603 | Acc: 0.7694 | F1: 0.6975\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1493,  414,  450,  115]))\n",
      "Epoch 39 | Time: 49.9s\n",
      "  Train Loss: 0.4758 | Acc: 0.7782 | F1: 0.7488\n",
      "  Val   Loss: 0.7130 | Acc: 0.7516 | F1: 0.6691\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1358,  480,  485,  149]))\n",
      "Epoch 40 | Time: 49.8s\n",
      "  Train Loss: 0.4661 | Acc: 0.7813 | F1: 0.7544\n",
      "  Val   Loss: 0.6900 | Acc: 0.7273 | F1: 0.6527\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1456,  251,  614,  151]))\n",
      "Epoch 41 | Time: 49.7s\n",
      "  Train Loss: 0.4559 | Acc: 0.7855 | F1: 0.7639\n",
      "  Val   Loss: 0.6618 | Acc: 0.7824 | F1: 0.7147\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1432,  350,  550,  140]))\n",
      "Epoch 42 | Time: 49.8s\n",
      "  Train Loss: 0.4514 | Acc: 0.7868 | F1: 0.7623\n",
      "  Val   Loss: 0.6591 | Acc: 0.7642 | F1: 0.6905\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1451,  318,  567,  136]))\n",
      "Epoch 43 | Time: 49.8s\n",
      "  Train Loss: 0.4468 | Acc: 0.7934 | F1: 0.7704\n",
      "  Val   Loss: 0.6622 | Acc: 0.7779 | F1: 0.7101\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1382,  373,  577,  140]))\n",
      "Epoch 44 | Time: 49.7s\n",
      "  Train Loss: 0.4391 | Acc: 0.7899 | F1: 0.7735\n",
      "  Val   Loss: 0.6762 | Acc: 0.7625 | F1: 0.6939\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1438,  336,  564,  134]))\n",
      "Epoch 45 | Time: 49.8s\n",
      "  Train Loss: 0.4318 | Acc: 0.7934 | F1: 0.7760\n",
      "  Val   Loss: 0.6869 | Acc: 0.7731 | F1: 0.7039\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1449,  318,  581,  124]))\n",
      "Epoch 46 | Time: 49.9s\n",
      "  Train Loss: 0.4277 | Acc: 0.7962 | F1: 0.7825\n",
      "  Val   Loss: 0.7067 | Acc: 0.7816 | F1: 0.7168\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1432,  316,  602,  122]))\n",
      "Epoch 47 | Time: 49.7s\n",
      "  Train Loss: 0.4176 | Acc: 0.8014 | F1: 0.7879\n",
      "  Val   Loss: 0.7139 | Acc: 0.7791 | F1: 0.7150\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1434,  306,  608,  124]))\n",
      "Epoch 48 | Time: 49.5s\n",
      "  Train Loss: 0.4162 | Acc: 0.7998 | F1: 0.7863\n",
      "  Val   Loss: 0.7005 | Acc: 0.7807 | F1: 0.7165\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1433,  323,  583,  133]))\n",
      "Epoch 49 | Time: 49.7s\n",
      "  Train Loss: 0.4238 | Acc: 0.7967 | F1: 0.7823\n",
      "  Val   Loss: 0.6839 | Acc: 0.7739 | F1: 0.7046\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([1420,  324,  595,  133]))\n",
      "Epoch 50 | Time: 49.2s\n",
      "  Train Loss: 0.4272 | Acc: 0.7983 | F1: 0.7837\n",
      "  Val   Loss: 0.6791 | Acc: 0.7751 | F1: 0.7061\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from src.parser import read_zip_binary\n",
    "from src.train import train_model\n",
    "from src.ecg_dataset import ECGDataset, prep_batch\n",
    "from src.stft_baseline import BaselineSTFTModel\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from src.split import create_stratified_split\n",
    "\n",
    "X_orig = read_zip_binary(\"../data/X_train.zip\")     #list of 1D numpy arrays\n",
    "y_orig = pd.read_csv(\"../data/y_train.csv\", header=None, names=[\"y\"])\n",
    "\n",
    "# Make compressed copies at 80% length\n",
    "X_comp = [ time_compress(s, rate=0.8) for s in X_orig ]\n",
    "\n",
    "# Concatenate signals & labels\n",
    "X_all = X_orig + X_comp\n",
    "y_all = pd.concat([y_orig, y_orig], ignore_index=True)\n",
    "\n",
    "# Stratified split on the _augmented_ set\n",
    "train_idx, val_idx = create_stratified_split(X_all, y_all, val_size=0.2, seed=343)\n",
    "\n",
    "# Build datasets & loaders\n",
    "train_dataset = ECGDataset(X_all, y_all, indices=train_idx)\n",
    "val_dataset   = ECGDataset(X_all, y_all, indices=val_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True,  collate_fn=prep_batch)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=16, shuffle=False, collate_fn=prep_batch)\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "# models is baseline stft model right now\n",
    "model = BaselineSTFTModel().to(device)\n",
    "\n",
    "\n",
    "# weights to avoid one class collapse\n",
    "weights = compute_class_weight(class_weight=\"balanced\", classes=np.array([0, 1, 2, 3]), y=y_all[\"y\"])\n",
    "weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# Learning rate not so important since we use OneCycleLR scheduler\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=5e-4,\n",
    "    weight_decay=1e-4  # L2 regularization to prevent overfitting\n",
    ")\n",
    "\n",
    "# train model\n",
    "model, history = train_model(model, train_loader, val_loader, optimizer, loss_fn, device, num_epochs=50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (arch-ml-venv)",
   "language": "python",
   "name": "arch-ml-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
