{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4cc5ed0",
   "metadata": {},
   "source": [
    "## Task 4 — Data Reduction (15 points)\n",
    "\n",
    "In this task, we reduce the size of the training dataset (originally ~62 MB zipped) while preserving classification performance. The objective is to produce reduced versions targeting 50%, 25%, and 10% of the original size.\n",
    "\n",
    "Our reduction pipeline includes:\n",
    "- **Fourier Transform**: Keeps only the top energy frequencies (`compress_fourier`)\n",
    "- **Piecewise Linear Approximation (PLA)**: Removes redundant values based on a maximum error threshold (`compress_pla`)\n",
    "- **Custom serialization**: Writes compressed signals to `.bin` files compatible with the original parser\n",
    "\n",
    "The following parameter combinations were used (obtained mostly via trial and error):\n",
    "- **50%**: `keep_ratio = 0.03`, `max_error = 15.0`\n",
    "- **25%**: `keep_ratio = 0.01`, `max_error = 22.0`\n",
    "- **10%**: `keep_ratio = 0.003`, `max_error = 35.0`\n",
    "\n",
    "* Assuming the original data resides in the folder called `data` located at the parent directory w.r.t. this notebook, the reduced training data files also will be located under that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc15dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2299831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "def compress_fourier(signal: np.ndarray, keep_ratio: float = 0.05) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    the main idea of this function is basically lossy compression of signals\n",
    "    using FT with thresholding\n",
    "    - transform signals into freq domain using FFT\n",
    "    - keep freq components which has highest magnitudes and zero the rest (thresholding)\n",
    "    - transform signals back into time domain using inverse FFT\n",
    "    \"\"\"\n",
    "    coeffs = fft(signal)    # apply fft to signal\n",
    "    magnitude = np.abs(coeffs)  \n",
    "    threshold = np.quantile(magnitude, 1 - keep_ratio)  # keep only *keep ratio* amount of signals (w.r.t. magnitudes)\n",
    "    coeffs[magnitude < threshold] = 0   # masking (thresholding)\n",
    "    return np.real(ifft(coeffs)).astype(np.int16)   # back into time domain using IFFT (we only need real part)\n",
    "\n",
    "def compress_pla(signal: np.ndarray, max_error: float = 10.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    apply piecewise linear approximation\n",
    "    reduce the number of points in the signal by removing small fluctuations\n",
    "    max_error : maximum allowed difference between consecutive retained values If a new point does not \n",
    "    differ from the last one by more than this value, it gets discarded\n",
    "    \"\"\"\n",
    "\n",
    "    # algorithm works similar to sliding window technique\n",
    "    if len(signal) == 0:\n",
    "        return signal\n",
    "    output = [signal[0]]\n",
    "    last = signal[0]\n",
    "    for x in signal[1:]:\n",
    "        if abs(x - last) > max_error:\n",
    "            output.append(x)\n",
    "            last = x\n",
    "    return np.array(output, dtype=np.int16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8bb327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import zipfile\n",
    "from src.parser import read_zip_binary\n",
    "\n",
    "\n",
    "def write_reduced_bin(signals: list, out_bin_path: str):\n",
    "    \"\"\"\n",
    "    write a list of reduced signals (as np arrays) into a single binary .bin file\n",
    "    uses the same structure as the original training data\n",
    "    \"\"\"\n",
    "    with open(out_bin_path, \"wb\") as f:\n",
    "        for signal in signals:\n",
    "            f.write(struct.pack(\"i\", len(signal)))\n",
    "            f.write(signal.tobytes())\n",
    "\n",
    "def create_reduced_zip(input_zip: str, output_zip: str, keep_ratio: float = 0.05, max_error: float = 10.0):\n",
    "    \"\"\"\n",
    "    basically the main function that compresses the data w.r.t. above logic and create\n",
    "    corresponding zip files\n",
    "\n",
    "    - load signals from the original zip\n",
    "    - compress each signal using fourier and pla\n",
    "    - write the reduced signals to a binary (.bin) file,\n",
    "    - zip the .bin file\n",
    "    \"\"\"\n",
    "    # load original data\n",
    "    print(f\"Reading original data from {input_zip}...\")\n",
    "    print(f\"Initial file size: {round(os.path.getsize(input_zip) / (1024 * 1024),1)} MBs.\")\n",
    "    signals = read_zip_binary(input_zip)\n",
    "\n",
    "    # apply compression\n",
    "    print(\"Applying Fourier + PLA compression... (wait)\")\n",
    "    reduced_signals = []\n",
    "    for signal in signals:\n",
    "        s1 = compress_fourier(signal, keep_ratio=keep_ratio)\n",
    "        s2 = compress_pla(s1, max_error=max_error)\n",
    "        reduced_signals.append(s2)\n",
    "\n",
    "    # write reduced data to binary (.bin) file\n",
    "    bin_name = os.path.splitext(os.path.basename(output_zip))[0] + '.bin'\n",
    "    temp_bin_path = f\"./{bin_name}\"\n",
    "    write_reduced_bin(reduced_signals, temp_bin_path)\n",
    "\n",
    "    # zip the bin file\n",
    "    print(f\"Zipping to {output_zip}...\")\n",
    "    with zipfile.ZipFile(output_zip, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "        zf.write(temp_bin_path, arcname=bin_name)\n",
    "\n",
    "    os.remove(temp_bin_path)\n",
    "    print(f\"Done. Final size: {round(os.path.getsize(output_zip) / (1024 * 1024),1)} MBs. \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab33e3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading original data from ../data/X_train.zip...\n",
      "Initial file size: 61.2 MBs.\n",
      "Applying Fourier + PLA compression... (wait)\n",
      "Zipping to ../data/X_train_reduced_10.zip...\n",
      "Done. Final size: 6.2 MBs. \n",
      "\n",
      "Reading original data from ../data/X_train.zip...\n",
      "Initial file size: 61.2 MBs.\n",
      "Applying Fourier + PLA compression... (wait)\n",
      "Zipping to ../data/X_train_reduced_25.zip...\n",
      "Done. Final size: 15.9 MBs. \n",
      "\n",
      "Reading original data from ../data/X_train.zip...\n",
      "Initial file size: 61.2 MBs.\n",
      "Applying Fourier + PLA compression... (wait)\n",
      "Zipping to ../data/X_train_reduced_50.zip...\n",
      "Done. Final size: 30.2 MBs. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# input path (original)\n",
    "input_zip = \"../data/X_train.zip\"\n",
    "\n",
    "# target file paths (reduced)\n",
    "output_zip_10 = \"../data/X_train_reduced_10.zip\"\n",
    "output_zip_25 = \"../data/X_train_reduced_25.zip\"\n",
    "output_zip_50 = \"../data/X_train_reduced_50.zip\"\n",
    "\n",
    "# below is the main part of the compression that uses below logic\n",
    "# and here we control the compression/reduction strength\n",
    "\n",
    "# reduce to 10 percent\n",
    "create_reduced_zip(input_zip, output_zip_10, keep_ratio=0.003, max_error=35.0)\n",
    "\n",
    "# reduce to 25 percent\n",
    "create_reduced_zip(input_zip, output_zip_25, keep_ratio=0.01, max_error=22.0)\n",
    "\n",
    "# reduce to 50 percent\n",
    "create_reduced_zip(input_zip, output_zip_50, keep_ratio=0.03, max_error=15.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29acce04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 617 samples (10%) to ../data/stratified_subset_10.npy\n",
      "Saved 1544 samples (25%) to ../data/stratified_subset_25.npy\n",
      "Saved 3089 samples (50%) to ../data/stratified_subset_50.npy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load training labels\n",
    "y_train = pd.read_csv(\"../data/y_train.csv\", header=None)\n",
    "y_train.columns = [\"y\"]\n",
    "\n",
    "def save_stratified_indices(y_train, prefix=\"stratified_subset\", fractions=[0.1, 0.25, 0.5]):   # prefix ~path name\n",
    "    \"\"\"\n",
    "    idea is basically creating subset of training data (10% 25% 50%)\n",
    "    instead of creating additional zip folders of training data we\n",
    "    sample through training labels (y) and save the corresponding indices\n",
    "    into a .npy file. for further use, we can simply use advanced indexing\n",
    "    functionality of numpy (just as in 01_data_exploration notebook)\n",
    "    \"\"\"\n",
    "    y_train = np.array(y_train)\n",
    "    all_indices = np.arange(len(y_train))  # 0 to len-1\n",
    "\n",
    "    for frac in fractions:\n",
    "        strat_indices, _ = train_test_split(\n",
    "            all_indices,\n",
    "            train_size=frac,\n",
    "            stratify=y_train,\n",
    "            random_state=1  # assign random state for reproducability\n",
    "        )\n",
    "        filename = f\"{prefix}_{int(frac * 100)}.npy\"    # indices will reside in .npy files\n",
    "        np.save(filename, strat_indices)\n",
    "        print(f\"Saved {len(strat_indices)} samples ({frac*100:.0f}%) to {filename}\")\n",
    "\n",
    "save_stratified_indices(y_train, prefix=\"../data/stratified_subset\")\n",
    "\n",
    "# later on those indices in npy files will be used for performance comparison between\n",
    "# reduced datasets and stratified subsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc2fa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training on dataset: reduced_10% (../data/X_train_reduced_10.zip) ===\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([765, 159, 187, 125]))\n",
      "Epoch 01 | Time: 21.8s\n",
      "  Train Loss: 1.3706 | Acc: 0.3639 | F1: 0.2640\n",
      "  Val   Loss: 1.2637 | Acc: 0.4773 | F1: 0.3268\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([391, 494, 223, 128]))\n",
      "Epoch 02 | Time: 18.6s\n",
      "  Train Loss: 1.3157 | Acc: 0.3480 | F1: 0.2801\n",
      "  Val   Loss: 1.1927 | Acc: 0.3463 | F1: 0.3175\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([240,  99, 796, 101]))\n",
      "Epoch 03 | Time: 15.6s\n",
      "  Train Loss: 1.2873 | Acc: 0.3261 | F1: 0.2744\n",
      "  Val   Loss: 1.1822 | Acc: 0.3722 | F1: 0.3222\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([474, 397, 157, 208]))\n",
      "Epoch 04 | Time: 15.2s\n",
      "  Train Loss: 1.2718 | Acc: 0.3519 | F1: 0.2957\n",
      "  Val   Loss: 1.1567 | Acc: 0.3673 | F1: 0.3103\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([872, 102, 144, 118]))\n",
      "Epoch 05 | Time: 17.2s\n",
      "  Train Loss: 1.2619 | Acc: 0.3393 | F1: 0.2879\n",
      "  Val   Loss: 1.1612 | Acc: 0.5348 | F1: 0.3588\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([403, 564, 118, 151]))\n",
      "Epoch 06 | Time: 17.5s\n",
      "  Train Loss: 1.2572 | Acc: 0.3405 | F1: 0.2926\n",
      "  Val   Loss: 1.1586 | Acc: 0.3269 | F1: 0.2952\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([664, 229, 198, 145]))\n",
      "Epoch 07 | Time: 16.1s\n",
      "  Train Loss: 1.2441 | Acc: 0.3450 | F1: 0.2980\n",
      "  Val   Loss: 1.1529 | Acc: 0.4628 | F1: 0.3601\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([276, 338, 379, 243]))\n",
      "Epoch 08 | Time: 16.6s\n",
      "  Train Loss: 1.2601 | Acc: 0.3587 | F1: 0.3011\n",
      "  Val   Loss: 1.1468 | Acc: 0.3066 | F1: 0.2833\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([309, 366, 445, 116]))\n",
      "Epoch 09 | Time: 16.1s\n",
      "  Train Loss: 1.2514 | Acc: 0.3471 | F1: 0.2982\n",
      "  Val   Loss: 1.1515 | Acc: 0.3544 | F1: 0.3410\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([516, 244, 302, 174]))\n",
      "Epoch 10 | Time: 16.0s\n",
      "  Train Loss: 1.2449 | Acc: 0.3517 | F1: 0.3005\n",
      "  Val   Loss: 1.1331 | Acc: 0.3981 | F1: 0.3324\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([266, 316, 436, 218]))\n",
      "Epoch 11 | Time: 18.0s\n",
      "  Train Loss: 1.2622 | Acc: 0.3387 | F1: 0.2919\n",
      "  Val   Loss: 1.1427 | Acc: 0.3277 | F1: 0.2991\n",
      "Unique predictions: (array([0, 1, 2, 3]), array([141, 355, 511, 229]))\n",
      "Epoch 12 | Time: 16.8s\n",
      "  Train Loss: 1.2562 | Acc: 0.3373 | F1: 0.2906\n",
      "  Val   Loss: 1.1595 | Acc: 0.2880 | F1: 0.2724\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from src.parser import read_zip_binary\n",
    "from src.train import train_model\n",
    "from src.ecg_dataset import ECGDataset, prep_batch, prep_batch_noisy_shifting\n",
    "from src.stft_featured import BaselineSTFTFeaturedModel\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from src.split import create_stratified_split\n",
    "from src.time_warp import time_compress_stretch\n",
    "\n",
    "# Load labels once\n",
    "y_orig = pd.read_csv(\"../data/y_train.csv\", header=None, names=[\"y\"])\n",
    "\n",
    "# Define only the compressed datasets to evaluate\n",
    "dataset_configs = {\n",
    "    \"reduced_10%\": \"../data/X_train_reduced_10.zip\",\n",
    "    \"reduced_25%\": \"../data/X_train_reduced_25.zip\",\n",
    "    \"reduced_50%\": \"../data/X_train_reduced_50.zip\"\n",
    "}\n",
    "\n",
    "# Common hyperparameters\n",
    "rates      = [0.8, 0.9, 1.1, 1.2]\n",
    "batch_size = 16\n",
    "val_batch  = 32\n",
    "device     = \"cpu\"\n",
    "num_epochs = 50\n",
    "lr         = 5e-4\n",
    "wd         = 1e-4\n",
    "\n",
    "results = {}\n",
    "\n",
    "for label, zip_path in dataset_configs.items():\n",
    "    print(f\"\\n=== Training on dataset: {label} ({zip_path}) ===\")\n",
    "    # Read reduced signals\n",
    "    X_orig = read_zip_binary(zip_path)\n",
    "    \n",
    "    # Stratified split\n",
    "    train_idx, val_idx = create_stratified_split(X_orig, y_orig, val_size=0.2, seed=343)\n",
    "\n",
    "    # Build train set: original + time‑warp augment\n",
    "    X_train_base = [X_orig[i] for i in train_idx]\n",
    "    y_train_base = y_orig.iloc[train_idx].reset_index(drop=True)\n",
    "    X_train_aug  = [time_compress_stretch(s, rate=np.random.choice(rates)) for s in X_train_base]\n",
    "    X_train_all  = X_train_base + X_train_aug\n",
    "    y_train_all  = pd.concat([y_train_base, y_train_base], ignore_index=True)\n",
    "\n",
    "    # Build validation set: only originals\n",
    "    X_val = [X_orig[i] for i in val_idx]\n",
    "    y_val = y_orig.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    # Create datasets & loaders\n",
    "    train_ds = ECGDataset(X_train_all, y_train_all)\n",
    "    val_ds   = ECGDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
    "                              collate_fn=prep_batch_noisy_shifting)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=val_batch, shuffle=False,\n",
    "                              collate_fn=prep_batch)\n",
    "\n",
    "    # Initialize model, loss, optimizer\n",
    "    model = BaselineSTFTFeaturedModel().to(device)\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=np.unique(y_train_all[\"y\"]),\n",
    "        y=y_train_all[\"y\"]\n",
    "    )\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(\n",
    "        weight=torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    # Train & record history\n",
    "    model, history = train_model(\n",
    "        model, train_loader, val_loader,\n",
    "        optimizer, loss_fn, device, num_epochs=num_epochs\n",
    "    )\n",
    "    results[label] = history\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg-amls-venv",
   "language": "python",
   "name": "eeg-amls-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
